{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install krwordrank\n",
    "!pip install tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "# GPU RAM 상황에 따라서 batch_size = 2 ~ 10\n",
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "input_max_length = 1024\n",
    "output_max_length = 128\n",
    "\n",
    "def get_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2')\n",
    "    return tokenizer\n",
    "\n",
    "# 긴 문장 전용 요약 함수\n",
    "# 1. 기사 내 키워드에 대한 점수 dict 추출 함수\n",
    "# Input : 'article : 긴 기사 원문 string'\n",
    "# Output : '해당 기사의 keyword list'\n",
    "def extract_keyword_scores(article, min_count=2, max_length=3, verbose=False, stopwords=[]):\n",
    "    # 문장 단위로 나누기\n",
    "    sentences = article.split(\".\")\n",
    "\n",
    "    wordrank_extractor = KRWordRank(\n",
    "        min_count = min_count,   # 단어의 최소 출현 빈도수\n",
    "        max_length = max_length, # 단어의 최대 길이\n",
    "        verbose = verbose\n",
    "    )\n",
    "\n",
    "    keywords, rank, graph = wordrank_extractor.extract(sentences)\n",
    "    for stopword in stopwords:\n",
    "        keywords.pop(stopword, None)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# 2. 키워드 점수 dict를 이용해 기사 내의 문장들에 대해 중요도 점수를 계산하는 함수\n",
    "# Input : 'article : 긴 기사 원문 string, keyword_score : 1에서 생성된 키워드 스코어 list'\n",
    "# Output : '각 문장의 score list'\n",
    "def compute_sentence_scores(article, keyword_scores):\n",
    "    sentences = article.split(\".\")\n",
    "    sentence_scores = defaultdict(float)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, score in keyword_scores.items():\n",
    "            if word in sentence:\n",
    "                sentence_scores[sentence] += score\n",
    "\n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "# 3. 가장 낮은 점수를 가진 문장을 제거하면서 기사 전체 길이가 1024 이하가 되도록 만드는 함수\n",
    "# Input : '긴 기사 원문 string', '각 문장의 score list'\n",
    "# Output : '짧은 기사 원문 string'\n",
    "def trim_article(article, sentence_scores, max_length=1024):\n",
    "    sentences = article.split(\".\")\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    while len(tokenizer.encode(article)) > max_length:\n",
    "        # 가장 점수가 낮은 문장 찾기\n",
    "        try:\n",
    "            min_score_sentence = min(sentence_scores.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "            if min_score_sentence in sentences:\n",
    "                sentences.remove(min_score_sentence)\n",
    "                # 제거된 문장의 점수 정보도 제거\n",
    "                sentence_scores.pop(min_score_sentence)\n",
    "        except:\n",
    "            sentences = sentences[:len(sentences)-2]\n",
    "\n",
    "        # 기사 다시 조합\n",
    "        article = '.'.join(sentences)\n",
    "\n",
    "    return article\n",
    "\n",
    "# 4. 긴 문장의 기사의 길이를 줄여주는 함수\n",
    "# Input : '긴 기사 원문 string'\n",
    "# Output : '짧은 기사 원문 string'\n",
    "def summarize_article(article, max_length=1024, min_count=2, max_length_word=3, verbose=False, stopwords=[]):\n",
    "    # 1. 기사 내 키워드에 대한 점수 dict 추출\n",
    "    keyword_scores = extract_keyword_scores(article, min_count, max_length_word, verbose, stopwords=stopwords)\n",
    "\n",
    "    # 2. 키워드 점수 dict를 이용해 기사 내의 문장들에 대해 중요도 점수를 계산\n",
    "    sentence_scores = compute_sentence_scores(article, keyword_scores)\n",
    "\n",
    "    # 3. 가장 낮은 점수를 가진 문장을 제거하면서 기사 전체 길이가 1024 이하가 되도록 만드는 과정\n",
    "    summarized_article = trim_article(article, sentence_scores, max_length)\n",
    "\n",
    "    return summarized_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 dataset 은 \"content\" 와 \"summary\" 를 가짐\n",
    "db_dataset_path = \"/content/drive/MyDrive/Tune/data/DB_train_Final.json\"\n",
    "opensource_dataset_path = \"/content/drive/MyDrive/Tune/data/OS_train_Final.json\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# 모델, 토크나이저 가져옴\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "# 학습 데이터 오픈\n",
    "import json\n",
    "\n",
    "with open(db_dataset_path) as f:\n",
    "    db_dataset = json.load(f)\n",
    "\n",
    "with open(opensource_dataset_path) as f:\n",
    "    opensource_dataset = json.load(f)\n",
    "\n",
    "# 각 loss 별 가중치\n",
    "keyword_loss_weight = 1\n",
    "simple_loss_weight = 1\n",
    "\n",
    "# 모델 GPU 로 이동\n",
    "model.to(device)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 토크나이즈 한 다음, dataLoader 로 변경\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        content = item[\"content\"]\n",
    "        summary = item[\"summary\"]\n",
    "        keywords = item[\"keywords\"]\n",
    "\n",
    "        # content 에 '키워드 : 키워드 1, 키워드 2, ..., 키워드 5' 추가\n",
    "        keywords_prefix = '(키워드 : '\n",
    "        for i, kw in enumerate(keywords):\n",
    "            keywords_prefix += kw\n",
    "            if i != len(keywords) - 1:\n",
    "                keywords_prefix += ', '\n",
    "            else:\n",
    "                keywords_prefix += \")\"\n",
    "\n",
    "        content = keywords_prefix + content\n",
    "\n",
    "        while len(keywords) < 5:\n",
    "            keywords.append(\"\")\n",
    "\n",
    "        # 길이가 긴 데이터 처리\n",
    "        if(len(self.tokenizer.encode(content)) > 1024):\n",
    "            content = summarize_article(item[\"content\"])\n",
    "\n",
    "        inputs = self.tokenizer(content, truncation=True, padding=\"max_length\", max_length=input_max_length, return_tensors=\"pt\")\n",
    "        targets = self.tokenizer(summary, truncation=True, padding=\"max_length\", max_length=output_max_length, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        labels = targets['input_ids'].squeeze()\n",
    "\n",
    "        return input_ids, attention_mask, labels, keywords\n",
    "\n",
    "\n",
    "# 불필요 텍스트 학습 시에 제거\n",
    "# 광고, 언론사 정보 등이 이에 해당\n",
    "must_remove_char = [\";\", \"▶\", \"ⓒ\", \"©\", \"☞\", \"&\", \"★\", \"☆\", \"✩\", \"&nbsp\"]\n",
    "def remove_substrings(data_list, target_string):\n",
    "    substrings = set()\n",
    "    updated_list = []\n",
    "    for data in data_list:\n",
    "        s = data['content']\n",
    "        if target_string in s:\n",
    "            last_occurrence = s.rfind(target_string)\n",
    "            substring = s[last_occurrence + len(target_string):]\n",
    "            if substring and any(char in substring for char in must_remove_char):\n",
    "                substrings.add(substring)\n",
    "                s = s[:last_occurrence + len(target_string)]  # substring 제거\n",
    "            data['content'] = s\n",
    "        updated_list.append(data)\n",
    "    return substrings, updated_list\n",
    "_, db_dataset = remove_substrings(db_dataset, \"다.\")\n",
    "\n",
    "db_train, db_test = train_test_split(db_dataset, test_size=0.1, random_state=42)\n",
    "opensource_train, opensource_test = train_test_split(opensource_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "db_train_dataset = ArticleDataset(db_train, tokenizer)\n",
    "db_test_dataset = ArticleDataset(db_test, tokenizer)\n",
    "\n",
    "db_train_loader = DataLoader(db_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "db_test_loader = DataLoader(db_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "opensource_train_dataset = ArticleDataset(opensource_train, tokenizer)\n",
    "opensource_test_dataset = ArticleDataset(opensource_test, tokenizer)\n",
    "\n",
    "opensource_train_loader = DataLoader(opensource_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "opensource_test_loader = DataLoader(opensource_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 학습 스케줄링 파라미터\n",
    "total_steps = (len(db_train_loader) + len(opensource_train_loader)) * num_epochs\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "# Optimizer 및 Scheduler 설정\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, correct_bias=False)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "# 두 가지 loss function\n",
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "def simple_finetuning_loss(logits, labels):\n",
    "    return cross_entropy(logits.view(-1, logits.shape[-1]), labels[:,1:].view(-1))\n",
    "\n",
    "# keywords 에 들어가야 하는 것은 현재 batch 의 기사에 해당하는 키워드들\n",
    "# keywords = [[기사 1 키워드 5개], [기사 2 키워드 5개], ...]\n",
    "def keyword_loss_function(outputs, labels, attention_mask, keywords, tokenizer):\n",
    "    generated_summary_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "    generated_summary = tokenizer.batch_decode(generated_summary_ids, skip_special_tokens=True)\n",
    "\n",
    "    keyword_loss = 0\n",
    "    # batch 내의 summary 순회\n",
    "    for idx, summary in enumerate(generated_summary):\n",
    "        keyword_count = sum([1 for keyword in keywords[idx] if keyword in summary])\n",
    "        # 키워드가 4,5 포함되어있으면 0, 0,1,2,3 개면 1\n",
    "        # if(keyword_count == 5):\n",
    "        #     keyword_loss += 0\n",
    "        # elif(keyword_count == 4):\n",
    "        #     keyword_loss += 0\n",
    "        # elif(keyword_count == 3):\n",
    "        #     keyword_loss += 0\n",
    "        # elif(keyword_count == 2):\n",
    "        #     keyword_loss += 3\n",
    "        # elif(keyword_count == 1):\n",
    "        #     keyword_loss += 15\n",
    "        # elif(keyword_count == 0):\n",
    "        #     keyword_loss += 63\n",
    "        if(keyword_count == 5):\n",
    "            keyword_loss += 1\n",
    "        elif(keyword_count == 4):\n",
    "            keyword_loss += 1\n",
    "        elif(keyword_count == 3):\n",
    "            keyword_loss += 1.5\n",
    "        elif(keyword_count == 2):\n",
    "            keyword_loss += 2\n",
    "        elif(keyword_count == 1):\n",
    "            keyword_loss += 2.5\n",
    "        elif(keyword_count == 0):\n",
    "            keyword_loss += 5\n",
    "    keyword_loss /= len(generated_summary)\n",
    "\n",
    "    return keyword_loss\n",
    "\n",
    "# earlystopping 구현, 3 or 5 가 일반적으로 사용\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            # 모델 저장\n",
    "            save_directory = f\"/content/drive/MyDrive/Tune/save/{self.counter}\"\n",
    "\n",
    "            # 모델의 파라미터 저장\n",
    "            torch.save(model.state_dict(), f\"{save_directory}/model.pt\")\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss decreased ({self.best_score:.4f} --> {score:.4f}).\")\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "\n",
    "            # 모델 저장\n",
    "            save_directory = f\"/content/drive/MyDrive/Tune/save/{self.counter}\"\n",
    "\n",
    "            # 모델의 파라미터 저장\n",
    "            torch.save(model.state_dict(), f\"{save_directory}/model.pt\")\n",
    "\n",
    "            # tokenizer 저장\n",
    "            tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Patience counter: {self.counter} out of {self.patience}.\")\n",
    "            if self.counter >= self.patience:\n",
    "\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            # 모델 저장\n",
    "            save_directory = f\"/content/drive/MyDrive/Tune/save/{self.counter}\"\n",
    "\n",
    "            # 모델의 파라미터 저장\n",
    "            torch.save(model.state_dict(), f\"{save_directory}/model.pt\")\n",
    "\n",
    "early_stopping = EarlyStopping(verbose=True)\n",
    "\n",
    "\n",
    "# train 과 evaluate 정의\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_simple_loss = 0\n",
    "    total_keyword_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        inputs, attention_mask, labels, keywords = batch\n",
    "        # print(\"inputs:\", inputs)  # Debugging line\n",
    "        # print(\"attention_mask:\", attention_mask)  # Debugging line\n",
    "        # print(\"labels:\", labels)  # Debugging line\n",
    "        # print(keywords)\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=inputs, labels=labels, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        simple_loss = outputs.loss\n",
    "        keyword_loss = keyword_loss_function(outputs, labels, attention_mask, list(zip(*keywords)), tokenizer)\n",
    "\n",
    "        # print(simple_loss)\n",
    "        # print(keyword_loss)\n",
    "\n",
    "        # 전체 loss\n",
    "        batch_loss = simple_loss * keyword_loss\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "        # 상태를 자세히 보기 위해 저장\n",
    "        total_simple_loss += simple_loss.item()\n",
    "        total_keyword_loss += keyword_loss\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    return total_loss / len(dataloader), total_simple_loss / len(dataloader), total_keyword_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_simple_loss = 0\n",
    "    total_keyword_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            inputs, attention_mask, labels, keywords = batch\n",
    "\n",
    "            inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids=inputs, labels=labels, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            simple_loss = outputs.loss\n",
    "            keyword_loss = keyword_loss_function(outputs, labels, attention_mask, list(zip(*keywords)), tokenizer)\n",
    "\n",
    "            # 전체 loss\n",
    "            batch_loss = simple_loss_weight * simple_loss + keyword_loss_weight * keyword_loss\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "            # 상태를 자세히 보기 위해 저장\n",
    "            total_simple_loss += simple_loss.item()\n",
    "            total_keyword_loss += keyword_loss\n",
    "\n",
    "    return total_loss / len(dataloader), total_simple_loss / len(dataloader), total_keyword_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 코드\n",
    "# Early Stop 이 출력 될 시 -> save/0/ 의 model0.pt 를 사용\n",
    "# Early Stop 이 출력 되지 않고 모든 에폭에 대해 학습 완료 시 -> save 폴더의 model.pt 를 사용\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, s_loss, k_loss = train(model, opensource_train_loader, optimizer)\n",
    "    eval_loss, e_s_loss, e_k_loss = evaluate(model, opensource_test_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Opensource set\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Simple Loss: {s_loss:.4f}, Keyword loss : {k_loss:.4f}\")\n",
    "    print(f\"Evaluation Loss: {eval_loss:.4f}, Simple Loss: {e_s_loss:.4f}, Keyword loss : {e_k_loss:.4f}\")\n",
    "\n",
    "    train_loss, s_loss, k_loss = train(model, db_train_loader, optimizer)\n",
    "    eval_loss, e_s_loss, e_k_loss = evaluate(model, db_test_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, DB set\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Simple Loss: {s_loss:.4f}, Keyword loss : {k_loss:.4f}\")\n",
    "    print(f\"Evaluation Loss: {eval_loss:.4f}, Simple Loss: {e_s_loss:.4f}, Keyword loss : {e_k_loss:.4f}\")\n",
    "\n",
    "    early_stopping(eval_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "\n",
    "# 모델 저장\n",
    "save_directory = \"/content/drive/MyDrive/Tune/save\"\n",
    "\n",
    "# 모델의 파라미터 저장\n",
    "torch.save(model.state_dict(), f\"{save_directory}/model.pt\")\n",
    "\n",
    "# tokenizer 저장\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
